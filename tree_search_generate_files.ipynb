{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98ab5e05",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/finith/anaconda3/lib/python3.7/site-packages/psycopg2/__init__.py:144: UserWarning: The psycopg2 wheel package will be renamed from release 2.8; in order to keep installing from binary please use \"pip install psycopg2-binary\" instead. For details see: <http://initd.org/psycopg/docs/install.html#binary-install-from-pypi>.\n",
      "  \"\"\")\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "\n",
    "from joblib import load\n",
    "from math import exp,log\n",
    "from rdkit import Chem, DataStructs\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import metrics\n",
    "import numpy\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from rdkit.Chem import Draw\n",
    "from rdkit.Chem import AllChem, PandasTools\n",
    "from rdkit.Chem import rdMolDescriptors\n",
    "from rdkit.Chem import Descriptors\n",
    "from rdkit.Chem import SaltRemover\n",
    "\n",
    "from rdkit.Chem import PandasTools\n",
    "from rdkit.Chem import QED\n",
    "from rdkit.Chem import Fragments\n",
    "from rdkit import ML\n",
    "import argparse\n",
    "import psycopg2\n",
    "\n",
    "from sshtunnel import SSHTunnelForwarder\n",
    "\n",
    "import openpyxl\n",
    "\n",
    "from datetime import datetime\n",
    "from anytree import Node, RenderTree \n",
    "from anytree import Node, RenderTree, AsciiStyle, ZigZagGroupIter, findall,findall_by_attr\n",
    "import multiprocessing as mp\n",
    "from functools import partial\n",
    "from rdkit import RDLogger\n",
    "RDLogger.DisableLog('rdApp.*')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e34cfa",
   "metadata": {},
   "source": [
    "# UTIL FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c177656",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_SMILES_to_SMARTS(x):\n",
    "    mol = Chem.MolFromSmiles(x)\n",
    "    smarts = Chem.MolToSmarts(mol)\n",
    "    return smarts\n",
    "\n",
    "def convert_SMILES_to_MOL(x):\n",
    "    mol = Chem.MolFromSmiles(x)\n",
    "    remover = SaltRemover.SaltRemover(defnData=\"[Cl,Br,HCl]\")\n",
    "    mol = remover.StripMol(mol1)\n",
    "    return mol\n",
    "\n",
    "def convert_SMARTS_to_SMILES(x):\n",
    "    smarts = overall_df.loc[overall_df['SMARTS']==x, 'SMILES']\n",
    "    return smarts\n",
    "\n",
    "def node_search(level1, level0): \n",
    "        try:\n",
    "            mol = Chem.MolFromSmiles(level1)\n",
    "            patt = Chem.MolFromSmarts(level0)\n",
    "            hit_ats = int(mol.HasSubstructMatch(patt))\n",
    "\n",
    "            if hit_ats > 0:\n",
    "                    return level0\n",
    "            else:\n",
    "                    return 0\n",
    "                \n",
    "        except AttributeError :\n",
    "            #print('syntax error for smiles')\n",
    "            return 0\n",
    "                  \n",
    "        \n",
    "def rdkit_tanimoto(mol1, mol2):\n",
    "    try:\n",
    "        mol1 =  rdkit.Chem.MolFromSmiles(mol1)\n",
    "        mol2 =  rdkit.Chem.MolFromSmiles(mol2)\n",
    "        fp1 = rdkit.Chem.RDKFingerprint(mol1)\n",
    "        fp2 = rdkit.Chem.RDKFingerprint(mol2)\n",
    "\n",
    "        tanimoto_score = rdkit.DataStructs.TanimotoSimilarity(fp1,fp2)\n",
    "        return tanimoto_score\n",
    "    except:\n",
    "         return None\n",
    "        \n",
    "from ast import literal_eval\n",
    "def extended(x,y):\n",
    "    overall = []\n",
    "    overall.extend(x)\n",
    "    overall.append(y)\n",
    "    return overall\n",
    "\n",
    "from ast import literal_eval\n",
    "def literal(x):\n",
    "    #try:\n",
    "    return literal_eval(x)\n",
    "    #except:\n",
    "    #    return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e38296",
   "metadata": {},
   "source": [
    "# LOAD DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6750f356",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/finith/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:12: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  if sys.path[0] == '':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE HERE\n",
      "DONE HERE\n"
     ]
    }
   ],
   "source": [
    "# file structure \n",
    "\n",
    "# level 0 and level 1\n",
    "start0 = pd.read_csv('11182021_SMILES_SMARTS_Hierarchy/D1/D1_Node_0a_mono_unsub_NH_to_N_nodup.csv')\n",
    "start1 = pd.read_csv('11182021_SMILES_SMARTS_Hierarchy/D1/D1_Node_0a_1a_mono_sub_NH_to_N_nodup.csv')\n",
    "start2 = pd.read_csv('11182021_SMILES_SMARTS_Hierarchy/D1/D1_Node_0a_1b_bicyclic_unsub_NH_to_N_nodup.csv')\n",
    "#start1a = pd.read_csv('11182021_SMILES_SMARTS_Hierarchy/D1/D1_Node_0a_1a_2a_bicyclic_sub_NH_to_N_nodup.csv')\n",
    "start1 = start1.append(start2)\n",
    "\n",
    "# top right\n",
    "start43 = pd.read_csv('11182021_SMILES_SMARTS_Hierarchy/D2/D2_Node_0a_SMARTS_FGs_2_left_c.txt',sep='/t')\n",
    "start43_kids = pd.read_csv('11182021_SMILES_SMARTS_Hierarchy/D2/D2_Node_0a_1a_FGs_SMARTS_FGs_c.csv',sep=',')\n",
    "\n",
    "# middle block\n",
    "start27 = pd.read_csv('11182021_SMILES_SMARTS_Hierarchy/3_Ar_SMARTS_SMILES_linker_Ar/1_MM/D1_Node_0a_1c_MM_CN_all_Xe_enumeration__SMARTS_linkers_NH_to_N_nodup.csv')\n",
    "start27_kids = pd.read_csv('11182021_SMILES_SMARTS_Hierarchy/3_Ar_SMARTS_SMILES_linker_Ar/1_MM/D1_Node_0a_1c_2c_MM_CN_all_Xe_enumeration_SMILES_SMARTS_nodup_NH_to_N_nodup.csv')\n",
    "\n",
    "# wildcards 73  \n",
    "wild_73 = pd.read_csv('11182021_SMILES_SMARTS_Hierarchy/wildcards_mono_connodup_730.csv')\n",
    "wild_73k = pd.read_csv('11182021_SMILES_SMARTS_Hierarchy/wildcards_bicyclic_connodup_73191.csv')\n",
    "\n",
    "# wildcards wc_sub_aro - mono\n",
    "wild_7 = pd.read_csv('11182021_SMILES_SMARTS_Hierarchy/7_wildcards_final_files/WC_Sub_Aro/wildcard_sub_mono_Cl_H_aro.csv')\n",
    "wild_7['SMARTS_WC_sub_aro'] = wild_7['SMARTS_WC_sub_aro'].apply(lambda x : literal(x))\n",
    "wild_7['Combined'] = wild_7.apply(lambda x: extended(x['SMARTS_WC_sub_aro'], x['SMARTS_WC_sub']), axis=1)\n",
    "\n",
    "#\n",
    "wild_7_bi = pd.read_csv('11182021_SMILES_SMARTS_Hierarchy/7_wildcards_final_files/WC_Sub_Aro/wildcard_sub_bicyclic_Cl_H_aro_SMILES_H.csv')\n",
    "wild_7_bi['SMARTS_WC_sub_aro'] = wild_7_bi['SMARTS_WC_sub_aro'].apply(lambda x : literal(x))\n",
    "wild_7_bi['SMILES_H'] = wild_7_bi['SMILES_H'].apply(lambda x : literal(x))\n",
    "def return_val(x):\n",
    "    val = x[0]\n",
    "    return x[0]\n",
    "\n",
    "wild_7_bi['SMILES_H']= wild_7_bi['SMILES_H'].apply(return_val)\n",
    "wild_7_bi['Combined'] = wild_7_bi.apply(lambda x: extended(x['SMARTS_WC_sub_aro'], x['SMARTS_WC_sub']), axis=1)\n",
    "# smiles cl match\n",
    "# ask for aro and sub\n",
    "\n",
    "\n",
    "print('DONE HERE')\n",
    "# create overall_df for smarts to smiles conversion\n",
    "overall_df = pd.concat([start0, start1, start27, start27_kids, start43, start43_kids])\n",
    "overall_df = overall_df['SMILES'].fillna(overall_df['SMARTS'])\n",
    "\n",
    "print('DONE HERE')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51a2a94e",
   "metadata": {},
   "source": [
    "# TREE SEARCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d2d73c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tree_building_1():\n",
    "\n",
    "    list1 = start0['SMARTS'].tolist()\n",
    "    start_0 = Node('START')\n",
    "    \n",
    "    for i in range(len(list1)):\n",
    "        print(i)\n",
    "        a = Node(list1[i], parent=start_0)\n",
    "\n",
    "        start_smiles = start1['SMILES'].tolist()\n",
    "        pool = mp.Pool(processes = (mp.cpu_count()-1))\n",
    "        results = pool.map(partial(node_search, level0=list1[i]), start_smiles)\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "\n",
    "        start1['CHECK'] = results\n",
    "        # select smiles that have matches to list1[i]\n",
    "        temp = start1.loc[start1['CHECK']!=0, 'SMARTS' ]\n",
    "        temp = temp.drop_duplicates()\n",
    "        temp = temp.tolist()\n",
    "\n",
    "        keep_temp = []\n",
    "        if temp:\n",
    "            for j in range(len(temp)):\n",
    "                b = Node(temp[j], parent=a)\n",
    "\n",
    "\n",
    "    return start_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f3f24dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creates tree \n",
    "# 43 -> 114k\n",
    "\n",
    "def node_string_search(level1, level0): \n",
    "    return level0 in level1 \n",
    "\n",
    "def tree_building_2():\n",
    "    list1 = start43['SMARTS'].tolist()\n",
    "    start_043 = Node('START')\n",
    "    superset = []\n",
    "    for i in range(len(list1)):\n",
    "        \n",
    "        print(i)\n",
    "        a = Node(list1[i], parent=start_043)\n",
    "\n",
    "        start_smiles = start43_kids['SMARTS'].tolist()\n",
    "        pool = mp.Pool(processes = (mp.cpu_count()-1))\n",
    "        results = pool.map(partial(node_string_search, level0=list1[i]), start_smiles)\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "\n",
    "        start43_kids['CHECK'] = results\n",
    "        temp = start43_kids.loc[start43_kids['CHECK']==True, 'SMARTS' ]\n",
    "        temp = temp.drop_duplicates()\n",
    "        temp = temp.tolist()\n",
    "        \n",
    "        for j in temp:\n",
    "            if j not in superset:\n",
    "                superset.append(j)\n",
    "            else:\n",
    "                temp.remove(j)\n",
    "\n",
    "        if temp:\n",
    "            for j in range(len(temp)):\n",
    "                b = Node(temp[j], parent=a)\n",
    "\n",
    "        \n",
    "    return start_043"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b89103",
   "metadata": {},
   "source": [
    "# GENERATING FILES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "424635b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 23000 x 53000\n",
    "def tree_building_middleblock_1():\n",
    "    \n",
    "    list1 = start27['SMARTS'].tolist()\n",
    "    csv_list = []\n",
    "    start_027 = Node('START')\n",
    "    superset = []\n",
    "    \n",
    "    for i in range(len(list1)):\n",
    "        \n",
    "        print(i)\n",
    "        a = Node(list1[i], parent=start_027)\n",
    "\n",
    "        start_smiles = start27_kids['SMILES'].tolist()\n",
    "        pool = mp.Pool(processes = (mp.cpu_count()-1))\n",
    "        results = pool.map(partial(node_search, level0=list1[i]), start_smiles)\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "\n",
    "        start27_kids['CHECK'] = results\n",
    "        temp = start27_kids.loc[start27_kids['CHECK']!=0, 'SMARTS' ]\n",
    "\n",
    "        temp = temp.drop_duplicates()\n",
    "        temp = temp.tolist()\n",
    "        for j in temp:\n",
    "            if j not in superset:\n",
    "                superset.append(j)\n",
    "            else:\n",
    "                temp.remove(j)\n",
    "\n",
    "        if temp:\n",
    "            csv_list.append([list1[i], temp])\n",
    "            \n",
    "    csv_list = pd.DataFrame(csv_list)\n",
    "    csv_list.to_csv('middle_block_searches.csv')\n",
    "\n",
    "    return start_027"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f5946708",
   "metadata": {},
   "outputs": [],
   "source": [
    "#matches from 43 parent to 150k\n",
    "\n",
    "def tree_building_243():\n",
    "    \n",
    "    list1 = start43['SMARTS'].tolist()\n",
    "    start243 = Node('START')\n",
    "    superset = []\n",
    "    csv_list = []\n",
    "    \n",
    "    for i in range(len(list1)):\n",
    "        print(i)\n",
    "        a = Node(list1[i], parent=start243)\n",
    "\n",
    "        start_smiles = start1['SMILES'].tolist()\n",
    "        pool = mp.Pool(processes = (mp.cpu_count()-1))\n",
    "        results = pool.map(partial(node_search, level0=list1[i]), start_smiles)\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "\n",
    "        start1['CHECK'] = results\n",
    "        temp = start1.loc[start1['CHECK']!=0, 'SMILES' ]\n",
    "        temp = temp.drop_duplicates()\n",
    "        temp = temp.tolist()\n",
    "        \n",
    "        for j in temp:\n",
    "            if j not in superset:\n",
    "                superset.append(j)\n",
    "            else:\n",
    "                temp.remove(j)\n",
    "                \n",
    "        csv_list.append([list1[i], temp])\n",
    "\n",
    "        if temp:\n",
    "            for j in range(len(temp)):\n",
    "                b = Node(temp[j], parent=a)\n",
    "\n",
    "        \n",
    "    df = pd.DataFrame(csv_list)\n",
    "    df.to_csv('43_to_150k_searches.csv')\n",
    "    return start243\n",
    "\n",
    "# find children from 150k and 114k that have 44 in common\n",
    "def tree_building_243_part2(node243):\n",
    "    \n",
    "    # find which of the 43 had matches\n",
    "    check_for_clustering = []\n",
    "    children_for_checking = []\n",
    "    for i in list(node243.children):\n",
    "        if len(list(i.children)) > 0: \n",
    "            check_for_clustering.append(i.name)\n",
    "            names = []\n",
    "            for j in i.children:\n",
    "                names.append(j.name)\n",
    "            children_for_checking.append(names)\n",
    "            \n",
    "    corresponding_values = pd.DataFrame()\n",
    "    corresponding_values['44'] = check_for_clustering\n",
    "    corresponding_values['150k'] = children_for_checking\n",
    "            \n",
    "            \n",
    "    # select the orignal children of those 43 (43->114k) -OLD \n",
    "    original_children = []\n",
    "    for i in node2.children:\n",
    "        if (i.name in check_for_clustering) and (len(list(i.children)) > 0):\n",
    "            selected_kids = []\n",
    "            for j in list(i.children):\n",
    "                selected_kids.append(j.name)\n",
    "            original_children.append([i.name,selected_kids])\n",
    "            \n",
    "    original_children = pd.DataFrame(original_children)\n",
    "    original_children.columns =['44', '114k']\n",
    "            \n",
    "    #44\n",
    "    #corresponding 150 for 44 \n",
    "    #corresponding 114 for 44\n",
    "    corresponding_values = pd.merge(corresponding_values, original_children, on='44')\n",
    "    \n",
    "    # check for duplicates\n",
    "    def check_dupes(x):\n",
    "        return list(set(x))\n",
    "    \n",
    "    corresponding_values['114k'] = corresponding_values['114k'].apply(lambda x: check_dupes(x))\n",
    "    corresponding_values['150k'] = corresponding_values['150k'].apply(lambda x: check_dupes(x))\n",
    "     \n",
    "    return corresponding_values\n",
    "\n",
    "\n",
    "# match selected 114k -> selected 150k\n",
    "def tree_building_part3(node):\n",
    "    list_main = node['114k'].tolist()\n",
    "    search_terms = node['150k'].tolist()\n",
    "    csv_list = []\n",
    "    \n",
    "    # for list1 in the list of 114, find corresponding matches of 114 to corresponding 150k\n",
    "    print(len(list_main),'first')\n",
    "    \n",
    "    for i in range(len(list_main)):\n",
    "        print(len(list_main[i]), 'second')\n",
    "                \n",
    "        searched = pd.DataFrame(search_terms[i])\n",
    "        start_smiles = search_terms[i]\n",
    "    \n",
    "        for j in list_main[i]:\n",
    "            pool = mp.Pool(processes = (mp.cpu_count()-1))\n",
    "            results = pool.map(partial(node_search, level0=j), start_smiles)\n",
    "            pool.close()\n",
    "            pool.join()\n",
    "            \n",
    "            searched['results'] = results\n",
    "            temp = searched.loc[searched['results']!= 0, 0].tolist()\n",
    "            csv_list.append([j, temp])\n",
    "            searched = searched[searched['results']==0]\n",
    "            start_smiles = searched[0].tolist()\n",
    "\n",
    "            \n",
    "    csv_list = pd.DataFrame(csv_list)\n",
    "    csv_list.to_csv('114k_150k.csv')\n",
    "    \n",
    "    return csv_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ee7d348c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 52 -> 730 searches \n",
    "def tree_building_52_730():\n",
    "    \n",
    "    list1 = wild_73['SMARTS'].tolist()\n",
    "    csv_list = []\n",
    "    start_073 = Node('START')\n",
    "    superset = []\n",
    "    \n",
    "    for i in range(len(list1)):\n",
    "        print(i)\n",
    "        a = Node(list1[i], parent=start_073)\n",
    "\n",
    "        start_smiles = start0['SMILES'].tolist()\n",
    "        pool = mp.Pool(processes = (mp.cpu_count()-1))\n",
    "        results = pool.map(partial(node_search, level0=list1[i]), start_smiles)\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "\n",
    "        start0['CHECK'] = results\n",
    "        temp = start0.loc[start0['CHECK']!=0, 'SMILES' ]\n",
    "\n",
    "        temp = temp.drop_duplicates()\n",
    "        temp = temp.tolist()\n",
    "        for j in temp:\n",
    "            if j not in superset:\n",
    "                superset.append(j)\n",
    "            else:\n",
    "                temp.remove(j)\n",
    "\n",
    "        if temp:\n",
    "            for j in temp:\n",
    "                csv_list.append([j, list1[i]])\n",
    "            \n",
    "    csv_list = pd.DataFrame(csv_list)\n",
    "    csv_list.to_csv('searched_730.csv')\n",
    "\n",
    "    return start_073"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "15d47bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1400 -> 73k searches \n",
    "def tree_building_1400_73k():\n",
    "    \n",
    "    list1 = wild_73k['SMARTS'].tolist()\n",
    "    csv_list = []\n",
    "    start_073k = Node('START')\n",
    "    superset = []\n",
    "    \n",
    "    for i in range(len(list1)):\n",
    "        print(i)\n",
    "        a = Node(list1[i], parent=start_073k)\n",
    "\n",
    "        start_smiles = start2['SMILES'].tolist()\n",
    "        pool = mp.Pool(processes = (mp.cpu_count()-1))\n",
    "        results = pool.map(partial(node_search, level0=list1[i]), start_smiles)\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "\n",
    "        start2['CHECK'] = results\n",
    "        temp = start2.loc[start2['CHECK']!=0, 'SMARTS' ]\n",
    "\n",
    "        temp = temp.drop_duplicates()\n",
    "        temp = temp.tolist()\n",
    "        for j in temp:\n",
    "            if j not in superset:\n",
    "                superset.append(j)\n",
    "            else:\n",
    "                temp.remove(j)\n",
    "\n",
    "        if temp:\n",
    "            for j in temp:\n",
    "                csv_list.append([j, list1[i]])\n",
    "            \n",
    "    csv_list = pd.DataFrame(csv_list)\n",
    "    csv_list.to_csv('searched_730k.csv')\n",
    "\n",
    "    return start_073"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "91629954",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 150k -> 832 wc_sub and wc_sub_aro \n",
    "\n",
    "def tree_building_150k_832():\n",
    "    \n",
    "    list1 = wild_7['SMARTS_WC_sub'].tolist()\n",
    "    list2 = wild_7['SMARTS_WC_sub_aro'].tolist()\n",
    "    csv_list = []\n",
    "    \n",
    "    for i in range(len(list1)):\n",
    "        print(i)\n",
    "\n",
    "        start_smiles = start1['SMILES'].tolist()\n",
    "        pool = mp.Pool(processes = (mp.cpu_count()-1))\n",
    "        results = pool.map(partial(node_search, level0=list1[i]), start_smiles)\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "\n",
    "        start1['CHECK'] = results\n",
    "        temp = start1.loc[start1['CHECK']!=0, 'SMILES' ]\n",
    "\n",
    "        temp = temp.drop_duplicates()\n",
    "        temp = temp.tolist()\n",
    "\n",
    "        if temp:\n",
    "            csv_list.append([temp, list1[i], list2[i]])\n",
    "            \n",
    "    csv_list = pd.DataFrame(csv_list)\n",
    "    csv_list.to_csv('searched_7_aro_sub.csv')\n",
    "\n",
    "    return csv_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "685096eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.4k -> 45k smarts_H \n",
    "def tree_building_1400_45k():\n",
    "    \n",
    "    list1 = wild_7_bi['SMARTS_H'].tolist()\n",
    "    list2 = wild_7_bi['Combined'].tolist()\n",
    "    csv_list = []\n",
    "    superset = []\n",
    "\n",
    "    for i in range(len(list1)):\n",
    "        \n",
    "            print(i)\n",
    "\n",
    "            start_smiles = start2['SMILES'].tolist()\n",
    "            pool = mp.Pool(processes = (mp.cpu_count()-1))\n",
    "            results = pool.map(partial(node_search, level0=list1[i]), start_smiles)\n",
    "            pool.close()\n",
    "            pool.join()\n",
    "\n",
    "            start2['CHECK'] = results\n",
    "            temp = start2.loc[start2['CHECK']!=0, 'SMILES' ].tolist()\n",
    "            print(len(temp))\n",
    "\n",
    "            if temp :\n",
    "                # second check\n",
    "                subset = list2[i]\n",
    "                for j in range(len(subset)): \n",
    "\n",
    "                    start_smiles2 = temp.copy()\n",
    "                    temp2 = pd.DataFrame(start_smiles2)\n",
    "\n",
    "                    pool = mp.Pool(processes = (mp.cpu_count()-1))\n",
    "                    results = pool.map(partial(node_search, level0=subset[j]), start_smiles2)\n",
    "                    pool.close()\n",
    "                    pool.join()\n",
    "\n",
    "                    temp2['CHECK'] = results\n",
    "                    temp2 = temp2.loc[temp2['CHECK']!=0, 0].tolist()\n",
    "\n",
    "                    if temp2:\n",
    "                        print(len(temp2))\n",
    "                        csv_list.append([list1[i], subset[j], temp2])\n",
    "                        \n",
    "                print('YES')\n",
    "                csv_list.append([list1[i],'None', temp])\n",
    "\n",
    "\n",
    "    csv_list = pd.DataFrame(csv_list)\n",
    "    csv_list.to_csv('searched_1k_45k.csv')\n",
    "\n",
    "    return csv_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6308d010",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.4k -> 45k smarts_H \n",
    "def tree_building_1400_45k_new():\n",
    "    \n",
    "    list1 = start2['SMARTS'].tolist()\n",
    "    list2 = wild_7_bi['SMILES_Cl'].tolist()\n",
    "    list3 = wild_7_bi['Combined'].tolist()\n",
    "    \n",
    "    csv_list = []\n",
    "    superset = []\n",
    "\n",
    "    for i in range(len(list1)):\n",
    "        \n",
    "            for j in range(len(list2)):\n",
    "                \n",
    "                if node_search(list2[i], list1[i]) != 0:\n",
    "                    print('YES')\n",
    "                    start_smiles = list3[i]\n",
    "                    pool = mp.Pool(processes = (mp.cpu_count()-1))\n",
    "                    results = pool.map(partial(node_search, level0=list1[i]), start_smiles)\n",
    "                    pool.close()\n",
    "                    pool.join()\n",
    "                    \n",
    "                    start = pd.DataFrame(list3[i])\n",
    "                    start['CHECK'] = results\n",
    "                    temp = start.loc[start['CHECK']!=0, 0 ].tolist()\n",
    "                    print(len(temp))\n",
    "                    \n",
    "                    if temp:\n",
    "                        csv_list.append(list1[i], temp)\n",
    "\n",
    "    csv_list = pd.DataFrame(csv_list)\n",
    "    csv_list.to_csv('searched_1k_45k.csv')\n",
    "\n",
    "    return csv_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bcddd66",
   "metadata": {},
   "source": [
    "# RUN TO GENERATE FILES "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c4e19fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 39->150k\n",
    "node1 = tree_building_1()\n",
    "\n",
    "#43-> 114k\n",
    "node2 = tree_building_2()\n",
    "\n",
    "# node1 and node2 need to be created before running these, 150k -> 114k\n",
    "part1 = tree_building_243()\n",
    "tree_df = tree_building_243_part2(part1)\n",
    "clustering_node = tree_building_part3(tree_df)\n",
    "\n",
    "# 2300 x 53000 middleblock \n",
    "tree_23 = tree_building_middleblock_1()\n",
    "\n",
    "# 52 -> 730 \n",
    "wild1 = tree_building_52_730()\n",
    "\n",
    "# 1400 -> 73k \n",
    "wild2 = tree_building_1400_73k()\n",
    "\n",
    "# 150k -> 832 \n",
    "tree_building_150k_832()\n",
    "\n",
    "# 1.4k -> 45000\n",
    "tree_building_1400_45k()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
